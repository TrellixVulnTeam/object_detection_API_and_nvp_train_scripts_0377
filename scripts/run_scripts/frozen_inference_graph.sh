# =======================================================================================
# frozen inference graph
# input: model.ckpt-n, which is generated by model training
# output: frozen_inference_graph.pb in frozen_inference_graph_n folder
# =======================================================================================

if [ ! ${2} ]; then
    echo "Error: frozen_inference_graph.sh, please enter steps and root path"
    exit
fi

num_steps=${1}
root_path=${2}
input_path="${root_path}scripts/input_config/"
output_path="${root_path}scripts/output/"

rm -rf ${output_path}/frozen_inference_graph/frozen_inference_graph_${num_steps}/*
# python3, need GPU to generate graph
source activate tensorflow-gpu
CUDA_VISIBLE_DEVICES="1" \
python ${root_path}models/research/object_detection/export_inference_graph.py \
    --input_type image_tensor \
    --pipeline_config_path ${input_path}ssd_mobilenet_v1_nvp.config \
    --trained_checkpoint_prefix ${output_path}train_logs/model.ckpt-${num_steps} \
    --output_directory ${output_path}frozen_inference_graph/frozen_inference_graph_${num_steps}
